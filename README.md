# File de-dup

CLI command to find duplicate files in a given directory.

## Usage

```
Arguments which can be passed to the tool to find de-dup of files

Usage: file-de-dup [OPTIONS] --full-path <FULL_PATH>

Options:
  -f, --full-path <FULL_PATH>
          Full path under which the files would be checked for de-dup
  -r, --recursive
          Check for files recursively. Default value is false
  -t, --filter-file-types <FILTER_FILE_TYPES>...
          Filter for given file types. If set to None then the tool runs for all of the file types
  -n, --num-threads <NUM_THREADS>
          Number of threads the tool should use to run the tool. If no value provided then we would use 3x number of cores.
  -c, --do-full-comparison
          Do a full comparison for the file which would be a byte by byte comparison. By default the tool would do a byte-by-byte comparison but if specified otherwise then the tool can use the checksum to evaluate the duplicate of files. Useful in cases we want to do a quick check of the files and we are aware of the content of the files by the name
  -h, --help
          Print help
  -V, --version
          Print version
```

*Features*

- Option to filter by file type.
- Option to check for duplicates recursively within the directory.
- Provides an option to perform a full byte-by-byte comparison to check for duplicates. By default, it uses the CRC64 checksum of the file to detect duplicates.

## Sample output result 

```
Duplicates of file1:
  - "/home/mukilesh/test_data/test_6/test_62/test300.c"
  - "/home/mukilesh/test_data/test_6/test_61/test300.c"
  - "/home/mukilesh/test_data/test_5/test200.c"
  - "/home/mukilesh/test_data/test_6/test209.c"
  - "/home/mukilesh/test_data/test10.c"
Duplicates of file2:
  - "/home/mukilesh/test_data/test1.c"
  - "/home/mukilesh/test_data/test_6/test_62/test999.c"
  - "/home/mukilesh/test_data/test_5/test20.c"
  - "/home/mukilesh/test_data/test_6/test102.c"
  - "/home/mukilesh/test_data/test_6/test_62/test_61/test1.c"
Duplicates of file3:
  - "/home/mukilesh/test_data/test_5/test19.c"
  - "/home/mukilesh/test_data/test.c"
  - "/home/mukilesh/test_data/test_6/test_61/test.c"
  - "/home/mukilesh/test_data/test_6/test_61/test_62/test102.c"
  - "/home/mukilesh/test_data/test_5/test_10/test_11/test100.c"
  - "/home/mukilesh/test_data/test_6/test100.c"
  - "/home/mukilesh/test_data/test_5/test_10/test.c"
```

*Explaination:* There are no assumptions about file names, so the output assigns an arbitrary file name like file<index> and lists all of the duplicate files.

## Design

The tool uses a pipeline design to execute various stages in parallel. This makes it easier to extend the tool with new features by adding additional stages to the pipeline. It also provides a simple way to configure the stages based on the use case.

### Pipeline

   ┌─────────────┐       ┌─────────────┐       ┌─────────────┐       ┌─────────────┐
   │    Walk     │------>│  Aggregate  │------>│ Checksum    │------>│  Full comp  │
   │             │       │             │       │             │       │             │
   └─────────────┘       └─────────────┘       └─────────────┘       └─────────────┘
                                                     |                    |
                                                     └────────────────────└─────────> Report

The diagram above depicts the various stages of the pipeline, and the sections below provide a detailed explanation of each stage.

#### Walker

The initial stage of the pipeline walks the provided directory, either recursively or non-recursively, depending on the argument provided by the user. It uses the Rayon library to perform the recursive traversal. If it encounters a valid file (not a directory, symlink, etc.), it adds an entry to a channel, which is then consumed by the next stage in the pipeline.

#### Aggregator

Initial aggregation of the files generated by the walker stage. It aggregates files by type and size. As it processes files from the channel, it adds them to an internal `DashMap`. If the `AGGREGATION_LIMIT` is reached, the aggregated batch is sent to the next stage via a channel. This limit helps in cases where no duplicates are observed for a specific file and also enables better performance by processing batches in the next stage.

At the end, any remaining aggregated files are sent to the next stage via the channel.

#### Checksum

Calculates the CRC64 checksum for the aggregated list of files provided by the previous stage and groups them based on the checksum (u64). If byte-by-byte comparison is not enabled by the user, the newly aggregated list of files is directly added to the report. Otherwise, it is sent through a channel to be consumed by the next stage.

The checksum is computed by reading the file in 16KB chunks and feeding the data into the CRC64 digest.

#### File compare

This is the final stage of the pipeline, responsible for performing a byte-by-byte comparison on a set of files. The tool executes this stage only if explicitly enabled by the user, as it is an expensive operation. Since byte-by-byte comparison is expensive, it is deferred until the final list of potential duplicates is available.

This stage is more complex than the others. It begins by chunking the list of aggregated files and performing comparisons serially for each chunk (the reasoning behind chunking is explained below). For each chunk, the stage creates a memory map (mmap) for each file starting at offset 0, and spawns a number of worker threads along with a shared work channel.

Each thread processes a list of files that need to be compared byte-by-byte. It starts by comparing a byte slice of size `min(4096, file_size - 1)`. If all files in the group match at that offset, the comparison continues with the next section of the file. If differences are found, the files are grouped by their differing byte slices, and new work items are enqueued to the channel with the updated offset. Since the files have already been grouped by size, we expect the offset progression to remain aligned across all files in a given group.

At the beginning, the total number of files to be processed is calculated. This count is decremented as each file's offset reaches its end. Once all files have been fully compared, this count acts as a signal for threads to exit gracefully. The shared work channel ensures that work is evenly distributed among threads.

*Why Chunking?*

The reason for chunking the aggregated file list is that Linux imposes a limit on the number of mmaps a process can create by default. Chunking provides a simple and effective way to stay within this limit while still processing large sets of files efficiently.

### Report

A simple reporting system that maintains a list of duplicate files and displays it. Since multiple threads from various stages can add files to the report, it is protected by a Mutex, which is acceptable (from perf perspective) in this case as it only involves adding file names to the list.

## TODO

1. Add a progress bar.
2. Evaluate the option of making it async (using tokio) and benchmark the two designs.
